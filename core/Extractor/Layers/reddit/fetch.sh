#!/bin/bash
# Reddit Layer - è³‡æ–™æŠ“å–è…³æœ¬
#
# ä½¿ç”¨æ–¹å¼ï¼š
#   ./fetch.sh                    # å¾ product_queries.txt æŠ“å–
#   ./fetch.sh --query "AirPods"  # æŒ‡å®šç”¢å“åç¨±
#   ./fetch.sh --subreddits "headphones,audiophile"  # æŒ‡å®š subreddits
#
# ç’°å¢ƒè®Šæ•¸ï¼š
#   REDDIT_CLIENT_ID     - Reddit API Client ID
#   REDDIT_CLIENT_SECRET - Reddit API Client Secret

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../../../.." && pwd)"

# è¼‰å…¥å…±ç”¨å‡½å¼åº«
source "$PROJECT_ROOT/lib/args.sh"
source "$PROJECT_ROOT/lib/core.sh"

LAYER_NAME="reddit"
RAW_DIR="$PROJECT_ROOT/docs/Extractor/$LAYER_NAME/raw"
QUERIES_FILE="$SCRIPT_DIR/product_queries.txt"

# é è¨­å€¼
MAX_POSTS=50
RELEVANCE_THRESHOLD=0.7

# è§£æåƒæ•¸
QUERY=""
SUBREDDITS=""

while [[ $# -gt 0 ]]; do
  case "$1" in
    --query)
      QUERY="$2"
      shift 2
      ;;
    --subreddits)
      SUBREDDITS="$2"
      shift 2
      ;;
    --max-posts)
      MAX_POSTS="$2"
      shift 2
      ;;
    *)
      echo "æœªçŸ¥åƒæ•¸: $1" >&2
      exit 1
      ;;
  esac
done

# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
mkdir -p "$RAW_DIR"

echo "ğŸ” Reddit Layer: é–‹å§‹æŠ“å–"

# æª¢æŸ¥ Reddit API æ†‘è­‰
if [[ -z "${REDDIT_CLIENT_ID:-}" ]] || [[ -z "${REDDIT_CLIENT_SECRET:-}" ]]; then
  echo "âš ï¸  æœªè¨­å®š Reddit API æ†‘è­‰ï¼Œä½¿ç”¨ WebSearch fallback æ¨¡å¼"
  USE_API=false
else
  USE_API=true
fi

# é€²å…¥ scrapers ç›®éŒ„
cd "$PROJECT_ROOT/scrapers"

# å–®ä¸€æŸ¥è©¢æ¨¡å¼
if [[ -n "$QUERY" ]]; then
  echo "  ğŸ“ æŸ¥è©¢: $QUERY"

  ARGS="--query \"$QUERY\" --output \"$RAW_DIR\" --max-posts $MAX_POSTS"

  if [[ -n "$SUBREDDITS" ]]; then
    ARGS="$ARGS --subreddits \"$SUBREDDITS\""
  fi

  if [[ "$USE_API" == "true" ]]; then
    ARGS="$ARGS --use-api"
  fi

  eval "npx tsx src/reddit/scraper.ts $ARGS" || {
    echo "  âš ï¸ æŠ“å–å¤±æ•—: $QUERY"
  }

  echo "âœ… Reddit æŠ“å–å®Œæˆ"
  exit 0
fi

# æ‰¹æ¬¡æ¨¡å¼ï¼šå¾ product_queries.txt è®€å–
if [[ ! -f "$QUERIES_FILE" ]]; then
  echo "âŒ æ‰¾ä¸åˆ° $QUERIES_FILE" >&2
  exit 1
fi

# è¨ˆç®—æŸ¥è©¢æ•¸é‡
QUERY_COUNT=$(grep -cvE '^\s*(#|$)' "$QUERIES_FILE" 2>/dev/null || echo "0")
echo "  ğŸ“‹ å…± $QUERY_COUNT å€‹ç”¢å“æŸ¥è©¢"

CURRENT=0
while IFS= read -r line || [[ -n "$line" ]]; do
  # è·³éç©ºè¡Œå’Œè¨»è§£
  [[ -z "$line" || "$line" == \#* ]] && continue

  CURRENT=$((CURRENT + 1))
  echo "  [$CURRENT/$QUERY_COUNT] $line"

  ARGS="--query \"$line\" --output \"$RAW_DIR\" --max-posts $MAX_POSTS"

  if [[ "$USE_API" == "true" ]]; then
    ARGS="$ARGS --use-api"
  fi

  eval "npx tsx src/reddit/scraper.ts $ARGS" || {
    echo "    âš ï¸ æŠ“å–å¤±æ•—"
  }

  # éš¨æ©Ÿå»¶é² 5-15 ç§’ï¼Œé¿å…è§¸ç™¼é™æµ
  DELAY=$((RANDOM % 10 + 5))
  sleep $DELAY

done < "$QUERIES_FILE"

echo "âœ… Reddit æŠ“å–å®Œæˆ: $CURRENT å€‹ç”¢å“"
